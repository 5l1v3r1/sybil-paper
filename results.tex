\section{Evaluation and results}
\label{sec:results}
We now discuss the results we obtained by applying the techniques we presented
above on the datasets we presented in Section~\ref{sec:datasets}.  In addition,
we evaluate the performance of sybilhunter (Section~\ref{sec:performance}) and
characterize the Sybils we discovered (Section~\ref{sec:sybil_groups}).

We characterize the most interesting Sybils as we proceed and give an overview
of all we found in Table~\ref{tab:sybils}.

\subsection{Accuracy of nearest-neighbor search}

\mynote{How good is our algorithm ad detecting \texttt{MyFamily} members, or
malicious relays that we know belong together?}

We determine the precision $\mathcal{P}$ and recall $\mathcal{R}$ that are
defined as

$$\mathcal{P} = \frac{|\textit{Correctly identified Sybils}|}
{|\textit{Uncorrectly identified Sybils}|}$$

$$\mathcal{R} = \frac{|\textit{Correctly identified Sybils}|}
{|\textit{Correctly identified Sybils} \cap \textit{Unidentified Sybils}|}$$

Precision is defined as the amount of true positives over false positives and
recall is defined as the amount of true positives over over true positives plus
false negatives.

Note that, by taking the set of families as ground truth, we are overestimating
the power of our algorithm.  relay operators make an effort to make relays look
the same.


\subsection{Performance}
\label{sec:performance}

Used SSD for analysis, bottleneck was CPU.

Table~\ref{tab:exp-deployment}
\begin{table}[t]
	\centering
	\begin{tabular}{lccc}
	\textbf{Analysis} & \textbf{Frequency} & \textbf{Window} & \textbf{Run time} \\
	\hline
	Network churn & Hourly & One hour & $\sim$0.16s \\
	Uptime matrix & Daily & One month & $\sim$67s \\
	Fingerprint analysis & Daily & One month & $\sim$55s \\
	Nearest-neighbor & Daily & One hour & $\sim$17s \\
	Similarity matrix & Daily & One hour & 10s \\
	\end{tabular}
	\caption{Todo.}
	\label{tab:exp-deployment}
\end{table}


\mynote{How time consuming is calculation of the similarity matrix?  What about
other demanding analysis techniques?  Is it possible to run them in real-time
as new consensuses are published?}

\subsection{Churn rate analysis}
We determined the churn rates of two subsequent consensuses for all 69,133
consensuses.  There are 158 gaps in the archived data, so we ended up with
$68,975 \cdot 2 = 137,950$ churn values for both time series.
Figure~\ref{fig:churn-density} illustrates a density plot that shows the
distribution of all these churn values, 99.97\% of which are in the interval
$[0, 0.1]$.  Table~\ref{tab:churn-dist} gives an overview of our time series
statistics.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{diagrams/churn-density.pdf}
	\caption{Density of all churn values for new and gone relays.  For both
	time series, 37 churn rates over eight years exceeded 0.1.  The dotted
	vertical lines mark all outliers.}
	\label{fig:churn-density}
\end{figure}

\begin{table}[t]
	\centering
	\begin{tabular}{ccccccc}
	\textbf{Churn type} & \textbf{Min.} & \textbf{Median} & \textbf{Mean} & \textbf{Max.} \\
	\hline
	New & 0.000 & 0.026 & 0.029 & 0.319 \\
	Gone & 0.003 & 0.025 & 0.029 & 0.412 \\
	\end{tabular}
	\caption{Summary of the distribution of churn rates for all consensuses
	since 2007.}
	\label{tab:churn-dist}
\end{table}

Figure~\ref{fig:churn-density} further features 37 vertical dotted lines that
mark outliers above 0.1.  We manually inspected these outliers as well as the
churn rate time series, and now turn our attention to the interesting
occurrences we discovered.  Figure~\ref{fig:2008-08} illustrates the churn rates
for August 2008.  On August 19, 822 relays left the network, resulting in a
sudden spike of the churn rate, and a trend increase in the time series.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{diagrams/2008-08.pdf}
	\caption{On August 19, 822 relays suddenly left the Tor network, resulting
	in a churn rate spike, and a time series increase.}
	\label{fig:2008-08}
\end{figure}

spike on 2008-08-19 was caused by consensus method switch.
\begin{quote}
It *could* be related to the switch from consensus method 3 to 4 which happened
on on 2008-08-19:

        * If consensus-method 4 or later is in use, then routers that
          do not have the Running flag are not listed at all.
\end{quote}

\subsection{Uptimes}
We generated relay uptime illustrations for every month since 2007, resulting in
93 uptime visualizations.\footnote{All images are available online, but the URL
was redacted for anonymization.}  We now discuss a subset of these images that
contain particularly interesting patterns.

Figure~\ref{fig:2010-06-planetlab} shows June 2010, featuring a clear ``Sybil
block'' on the left side.  The Sybils belonged to a researcher who, as
documented by The Tor Project~\cite{progressreport}, started 512 Tor relays on
PlanetLab for research on scalability.  Our manual analysis could verify this.
The relays were easy to identify because their nicknames suggested that they
were hosted on PlanetLab, containing strings such as ``planetlab,'' ``planet,''
and ``plab.''  Note the small height of the Sybil block, indicating the the
relays were not online for a long time.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{diagrams/planetlab-uptimes.jpg}
	\caption{In June 2010, a researcher started 512 Tor relays on PlanetLab for,
		as The Tor Project documented, ``their research into cloud computing and
		scaling effects''~\cite{progressreport}.  As illustrated by the easily
		visible red bar on the left, the relays were only online for a short
		while.}
	\label{fig:2010-06-planetlab}
\end{figure}

Figure~\ref{fig:2012-08-steppattern} features a curious ``step pattern'' for
approximately 100 relays, all of which were located in Russia and Germany.  The
relays appeared in December 2011, and started exhibiting the diurnal step
pattern (nine hours uptime followed by 15 hours downtime) in March 2012.  All
relays had similar nicknames, consisting of eight seemingly randomly-generated
characters.  In April 2013, the relays finally disappeared.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{diagrams/2012-08.jpg}
	\caption{August 2012 featured a curious ``step pattern,'' caused by
	approximately 100 Sybils.  Out of 24 hours, the relays were online for only
	nine hours.}
	\label{fig:2012-08-steppattern}
\end{figure}

Figure~\ref{fig:2014-04-heartbleed} shows the effect of the Heartbleed
incident~\cite{Durumeric2014a} on the Tor network.  Several days after the
incident, The Tor Project decided to block all relays that haven't generated new
key pairs.  The large red block in the middle of the picture illustrates when
the biggest part of the block became active, rejecting approximately 1,700 Tor
relay fingerprints.
% $ wc -l dirauth-conf/approved-routers.d/bleeding-edges.conf
% 1779 dirauth-conf/approved-routers.d/bleeding-edges.conf

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{diagrams/heartbleed-uptimes.jpg}
	\caption{April 2014, the month the Heartbleed bug was discovered.
		The large block in the middle of the diagram happened because The
		Tor Project eventually rejected a large number of relays that did not
		change their keypairs in time.}
		\label{fig:2014-04-heartbleed}
\end{figure}

Figure~\ref{fig:2014-12-lizard} illustrates the largest Sybil group to date,
comprising 3,347 Tor relays that an attacker started in the Google cloud in
December 2014.  Because of its magnitude, the attack was spotted almost
instantly, and The Tor Project was quick to remove the offending relays nine
hours after the appeared.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{diagrams/lizard-uptimes.jpg}
	\caption{December 2014, when a group of people started several hundred Tor
	relays in the Google cloud.  The relays were only online for a small number
	of hours because they were promptly rejected by The Tor Project.}
	\label{fig:2014-12-lizard}
\end{figure}

\subsection{Fingerprint anomalies}
\label{sec:fingerprint-anomalies}
We determined how often all Tor relays changed their fingerprint from 2007 to
2015.  Figure~\ref{fig:fingerprints} illustrates the amount of fingerprints
(y-axis) we have observed for the 1,000 Tor relays (x-axis) that changed their
fingerprint the most.  All these relays changed their fingerprint at least ten
times.  Twenty one relays changed their fingerprint more than 100 times, and the
relay at right end of the distribution changed its fingerprint 936 times.  This
relay's nickname was ``openwrt,'' suggesting that it was a home router that was
rebooted regularly.  It was running from August 2010 to December 2010.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{diagrams/fingerprints.pdf}
	\caption{The amount of observed fingerprints for the 1,000 relays that
	changed their fingerprints the most.  Note the curious plateau in the shaded
	area between index 707 and 803---a Sybil group that changed their
	fingerprint exactly 24 times.}
	\label{fig:fingerprints}
\end{figure}

Figure~\ref{fig:fingerprints} further contains a peculiar plateau, shown in the
shaded area between index 707 and 803.  This plateau was caused by a group of
Sybils, hosted in Amazon EC2, that changed their fingerprint exactly 24 times.

% Many relays in Riseup's IP address space.
199.254.238.0/24
belonged to legacy VPN service, VPN users might have run Tor relays there.
IP addresses were also not permanent, which might have caused the churn.

\subsection{Sybil characterization}
\label{sec:sybil_groups}
Table~\ref{tab:sybils} contains all the Sybil groups we identified.  For every
group, we document when it first appeared in the Tor network, its ``name,''
size, and a description of its characteristics.

\begin{table*}[t]
\centering
\begin{tabular}{l c c p{10cm}}
\textbf{First seen} & \textbf{Group ID} & \textbf{\# of relays} & \textbf{Characteristics} \\
\hline
\ldots & \ldots & 6 & Disable STARTTLS for SMTP. \\
2015-07-10 & DenkoNet & 58 & Hosted on Amazon AWS.  Only online for one consensus. \\
2015-07-02 & cloudvps & 55 & \ldots \\
2015-06-29 & onion rewrite & 55 & Transparent onion URL rewriting. \\
2015-06-17 & 1jabberat & \ldots & \ldots \\
2015-06-05 & hsdirscanner2 & 105 & Scanning HSDirs. \\
2015-06-03 & abcd & 28 & Changing fingerprints. \\
2015-05-29 & facebook hsdirs & 6 & Became HSDirs responsible for facebook.  \\
2015-05-20 & hsdirscanner1 & 102 & Scanning HSDirs. \\
2015-04-22 & sigaint & 83 & Targeting (at least) sigaint.org. \\
2015-03-11 & bitcoin-redirect & 24 & Attacking Bitcoin sites by redirecting to own web server. \\
\ldots & default & many & Likely a Windows-powered botnet.  The group
features wide, geographical distribution, which is uncommon for typical Tor
relays. \\
% Shared: nickname, IP address, port, platform, version.
2014-12-30 & Anonpoke & 284 & \ldots\\
2014-12-26 & FuslVZTOR & 246 & The relays showed up only hours after the
LizardNSA incident. \\
2014-12-26 & LizardNSA & 3,347 & A hacker group publicly claimed to be
responsible for the attack.  All relays were hosted in the Google cloud and The
Tor Project removed them within hours. \\
2013-02-03 & AmazonEC2 & \ldots & Changed their fingerprint exactly 24 times. \\
2014-01-04 & fdcserver & \ldots & \ldots \\
2010-10-05 & trotsky & \ldots & \ldots \\
2010-06-26 & planetlab & 512 & According to a report from The Tor
Project~\cite{progressreport}, a researcher started these relays to learn more
about scalability effects.  The relays were online for approximately two days. \\
2008-09-26 & torism & 10 & \ldots \\
\end{tabular}
\caption{Sybil groups identified by our system.  Our raw data is available
online. \emph{URL redacted for anonymization.}}
\label{tab:sybils}
\end{table*}

\subsubsection{default}
This Sybil group, named after the shared nickname ``default,'' has been around
since 2011 (see Figure~\ref{fig:default-over-time}) and consists of
Windows-powered relays only.  All members run Tor in version 0.2.4.X, many
releases are older than Sep. 2014.  We extracted relays by filtering consensuses
for nicknames that are set to ``default,'' onion routing ports set to 443, and
directory ports set to 9030.  The group features high IP address churn.  For
Oct. 2015, we found ``default'' relays in 73 countries, with the top three
countries being Germany~(50\%), Russia~(8\%), and Austria~(7\%).  The majority
of these relays, however, has little uptime.
Figure~\ref{fig:default-sybils-uptime} shows the uptime matrix for ``default''
relays in Oct. 2015.  Many relays exhibit a weak diurnal pattern, suggesting
that they are powered off regularly.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{diagrams/default-over-time}
	\caption{The amount of relays that we deem part of the Sybil group
	``default'' over time.  The relays surfaced in Sep. 2011.}
	\label{fig:default-over-time}
\end{figure}

To get a better understanding of the amount of ``default'' relays over time, we
analyzed all consensuses, extracting the number of relays whose nickname was
``default,'' whose onion routing port was 443, and whose directory port was
9001.  We further smoothed the time series using a simple moving average with
window size X to highlight the underlying trend.  The resulting time series is
shown in Figure~\ref{fig:default-over-time}.

The above suggests that some of these relays are running without the owner's
knowledge.  The relays don't fit the pattern of Sefnit (a.k.a.
Mevade)~\cite{sefnit} and SKynet~{skynet}, two pieces of malware that use an
onion service as command and control server.  Nevertheless, we believe that
these could could be part of a botnet.

\subsubsection{LizardNSA}
All relays were hosted in the Google Cloud, and only online for nine hours,
until the directory authorities started rejecting them.  The majority of
machines were middle relays (96\%), but the attackers also started some exit
relays (4\%).  The Sybils were set up to be hidden service directories, but the
relays were taken offline before they were assigned the \texttt{HSDir} flag.  If
all relays would have obtained the \texttt{HSDir} flag in time, they would have
constituted almost 50\% of all hidden service directories; the median number of
hidden service directories on Dec. 26 was 3,551.

\subsubsection{FuslVZTOR}
All machines were middle relays and hosted in 212.38.181.0/24, a VPS provider's
network in the UK.  The directory authorities started rejecting the relays five
hours after they were first seen.  The relays advertized the default bandwidth
of 1 GiB/s and used seemingly randomly determined ports.  Other than happening
in parallel to the LizardNSA attack, there is no reason to believe that both
incidents are related.

\subsubsection{Anonpoke}
The relays were online for four hours until they were rejected.  All relays were
hosted by a VPS provider in the US, with the curious exception of a single relay
that was hosted in the UK, and running a different Tor version.  The relays
advertized the default bandwidth of 1 GiB/s on port 9001 and 9030.  All relays
were middle relays and running as directory mirror.  All Sybils were configured
to be a hidden service directory, but did not manage to get the flag in time.

\subsubsection{PlanetLab}
A set of relays that used a variation of the strings ``planet'', ``plab'',
``pl'', and ``planetlab'' as their nickname.  The relays' exit policy allowed
ports 6660--6667, but they did not get the \texttt{Exit} flag.  The Sybils were
online for three days and then removed by The Tor Project, as mentioned in a
blog post~\cite{planetlab}.  The blog post further says that the relays were run
by a researchers.

\subsubsection{Bitcoin}
The ones that are stealing bitcoins.

Look at blockchain and figure out how much they stole.
