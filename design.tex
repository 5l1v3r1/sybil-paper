\section{Data and design}
\label{sec:design}
We define Sybils in the Tor network as two or more relays that are controlled by
a single person or group of people.  Sybils per se do not have to be malicious;
a relay operator could simply have forgotten to configure her relays as a relay
family.  Such Sybils are no threat to the Tor network, which is why we refer to
them as \emph{benign Sybils}.  What we are interested in is \emph{malicious
Sybils} whose purpose is to deanonymize or otherwise harm Tor users.

To uncover malicious Sybils, we draw on two datasets---one publicly available
and one created by us.  Our detection methods are implemented in a tool, \sys,
which takes as input our two datasets and then attempts to expose Sybil groups,
as illustrated in Figure~\ref{fig:system}.  \Sys is implemented in Go and
consists of 2,300 lines of code.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{diagrams/architecture.pdf}
	\caption{\Sys's architecture.  Two datasets serve as input to
		\sys; consensuses and server descriptors, and malicious
		relays gathered with exitmap~\cite[\S~3.1]{Winter2014a}.}
	\label{fig:system}
\end{figure}

\subsection{Datasets}
\label{sec:datasets}
Figure~\ref{fig:system} shows how we use our two datasets.  Archived
consensuses and router descriptors (in short: descriptors) allow us to
(\emph{i}) restore past states of the Tor network, which \sys mines for Sybil
groups, and to (\emph{ii}) find ``partners in crime'' of malicious exit relays
that we discovered by running exitmap, a scanner for Tor exit relays that we
discuss below.

\subsubsection{Consensuses and router descriptors}
The consensus and descriptor dataset is publicly available on
CollecTor~\cite{collector}, an archiving service that is run by The Tor Project.
Some of the archived data dates back to 2004, allowing us to restore arbitrary
Tor network configurations from the last decade.  Not all of CollecTor's
archived data is relevant to our hunt for Sybils, though, which is why we only
analyze the following two:

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{diagrams/consensus-and-descriptor.pdf}
	\caption{Our primary dataset contains nine years worth of consensuses and
	router descriptors.}
	\label{fig:datasets}
\end{figure}

\paragraph{Descriptors} Tor relays and bridges periodically upload router
descriptors, which capture their configuration, to directory authorities.
Figure~\ref{fig:datasets} shows an example in the box to the right.  Relays
upload their descriptors no later than every 18 hours, or sooner, depending on
certain conditions.  Note that some information in router descriptors is not
verified by directory authorities.  Therefore, relays can spoof information such
as their operating system, Tor version, and uptime.

\paragraph{Consensuses} Each hour, the nine directory authorities vote on their
view of all Tor relays that are currently online.  The vote produces the
consensus, an authoritative list that comprises all running Tor relays,
represented as a set of router statuses.  Each router status in the consensus
contains basic information about Tor relays such as their bandwidth, flags, and
exit policy.  It also contains a pointer to the relay's descriptor, as shown in
Figure~\ref{fig:datasets}.  As of June 2016, consensuses contain approximately
7,000 router statuses, i.e., each hour, 7,000 router statuses are published,
and archived, by CollecTor.

Table~\ref{tab:collector-dataset} gives an overview of the size of our
consensus and descriptor archives.  We found it challenging to repeatedly
process these millions of files, amounting to more than 100 GiB of uncompressed
data, so we implemented a custom parser in Go~\cite{zoossh}.

\begin{table}[t]
\small
\centering
\begin{tabular}{l r r r}
\toprule
\textbf{Dataset} & \textbf{\# of files} & \textbf{Size} & \textbf{Time span} \\
\midrule
% find . -type f | wc -l
% du -b consensuses/
Consensuses & 72,061 & 51 GiB & 10/2007--01/2016 \\
Descriptors & 34,789,777 & 52 GiB & 12/2005--01/2016 \\
\bottomrule
\end{tabular}
\caption{An overview of our primary dataset; consensuses and server descriptors
since 2007 and 2005, respectively.}
\label{tab:collector-dataset}
\end{table}

\subsubsection{Malicious exit relays}
In addition to our publicly available and primary dataset, we collected
malicious exit relays over 18 months.  We call exit relays malicious if they
modify forwarded traffic in bad faith, e.g., to run man-in-the-middle attacks.
We add these relays to our dataset because they frequently \emph{surface in
groups}, as malicious Sybils, because an attacker runs the same attack on
several, physically distinct exit relays.  Winter et al.'s
work~\cite[\S~5.2]{Winter2014a} further showed that attackers make an effort to
stay under the radar, which is why we cannot only rely on active probing to
find such relays.  We also seek to find potential ``partners in crime'' of each
newly discovered malicious relay, which we discuss in
Section~\ref{sec:nearest-neighbor}.

We exposed malicious exit relays using Winter et al.'s exitmap
tool~\cite[\S~3.1]{Winter2014a}.  Exitmap is a Python-based scanning framework
for Tor exit relays.  Exitmap modules perform a network task that can then be
run over all exit relays.  One use case is HTTPS man-in-the-middle detection: A
module can fetch the certificate of a web server over all exit relays and then
compare its fingerprint with the expected, valid fingerprint.  Exposed attacks
are sometimes difficult to attribute because an attack can take place upstream
of the exit relay, e.g., at a malicious autonomous system.  However,
attribution is only a secondary concern.  Our primary concern is protecting Tor
users from harm, and we do not need to identify the culprit to do so.

In addition to using the original exitmap modules~\cite[\S~3.1]{Winter2014a},
we implemented modules that detect HTML and HTTP tampering by connecting to a
decoy server under our control, and flagging an exit relay as malicious if the
returned HTML or HTTP was modified, e.g., to inject data or redirect a user
over a transparent HTTP proxy.  Since we controlled the decoy server, we knew
what our Tor client should get in response.  Our modules ran periodically from
August 2014 to January 2016, and discovered 251 malicious exit relays whose
attacks are discussed in Appendix~\ref{sec:malicious-relays}.  We reported all
relays to The Tor Project, which subsequently blocked these relays.

\subsection{Threat model}
\label{sec:threat_model}
Most of this paper is about applying \sys to archived network data, but we can
also apply it to newly incoming data.  This puts us in an adversarial setting
as attackers can tune their Sybils to evade our system.  This is reflected in
our adversarial assumptions.  We assume that an adversary \emph{does} run more
than one Tor relay and exhibits redundancy in their relay configuration, or
uptime sequence.  An adversary further \emph{can} know how \sys's modules work,
run active or passive attacks, and make a limited effort to stay under the
radar, by diversifying parts of their configuration.  To detect Sybils,
however, our heuristics require \emph{some} redundancy.

\subsection{Analysis techniques}
\label{sec:techniques}
Having discussed our datasets and threat model, we now turn to presenting
techniques that can expose Sybils.  Our techniques are based on the insight
that Sybil relays frequently \emph{behave or appear similarly}.  Shared
configuration parameters such as port numbers and nicknames cause similar
appearance whereas Sybils behave similarly when they reboot simultaneously, or
exhibit identical quirks when relaying traffic.

\Sys can analyze (\emph{i}) historical network data, dating back to 2007;
(\emph{ii}) online data, to detect new Sybils as they join the network; and
(\emph{iii}) find relays that might be associated with previously discovered,
malicious relays.  Figure~\ref{fig:shr-internal} shows \sys's internal
architecture.  Tor network data first passes a filtering component that can be
used to inspect a subset of the data, e.g., only relays with a given IP address
or nickname.  The data is then forwarded to one or more modules that implement
an analysis technique.  These modules work independently, but share a data
structure to find suspicious relays that show up in more than one module.
Depending on the analysis technique, \sys's output is either CSV files or
images.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{diagrams/internal-architecture.pdf}
	\caption{\Sys's internal architecture.  After an optional filtering step,
	data is then passed on to one of three analysis modules that produce as
	output either CSV files or an image.}
	\label{fig:shr-internal}
\end{figure}

While developing \sys, we had to make many design decisions that we tackled by
drawing on the experience we gained by manually analyzing numerous Sybil
groups.  We iteratively improved our code and augmented it with new features
when we experienced operational shortcomings.

\subsubsection{Network churn}
\label{sec:churn-time-series}
The churn rate of a distributed system captures the rate of joining and leaving
network participants.  In the Tor network, these participants are relays.  An
unexpectedly high churn rate between two subsequent consensuses means that many
relays joined or left, which can reveal Sybils and other network issues because
many Sybil operators start and stop their Sybils at the same time, to ease
administration---they behave similarly.

The Tor Project is maintaining a Python script~\cite{doctor} that determines the
number of previously unobserved relay fingerprints in new consensuses.  If that
number is greater than or equal to the static threshold 50, the script sends an
e-mail alert.  We reimplemented the script in \sys and ran it over all archived
consensus documents, dating back to 2007.  The script raised 47 alerts in nine
years, all of which seemed to be true positives, i.e., they should be of
interest to The Tor Project.  The script did not raise false positives,
presumably because the median number of previously unseen fingerprints in a
consensus is only six---significantly below the conservative threshold of 50.
Yet, the threshold likely causes false negatives, but we cannot determine the
false negative rate because we lack ground truth.  In addition, The Tor
Project's script does not consider relays that left the network, does not
distinguish between relays with different flags, and does not adapt its
threshold as the network grows.  We now present an alternative approach that is
more flexible and robust.

We found that churn anomalies worthy of our attention range from \emph{flat
hills} (Figure~\ref{fig:flat-hill}) to \emph{sudden spikes}
(Figure~\ref{fig:sudden-spike}).  Flat hills can be a sign of an event that
affected a large number of relays, over many hours or days.  Such an event
happened shortly after the Heartbleed bug, when The Tor Project asked relay
operators to generate new keys.  Relay operators acted gradually, most within
two days.  Sudden spikes can happen if an attacker adds many relays, all at
once.  These are mere examples, however; the shape of a time series cannot tell
us anything about the nature of the underlying incident.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{diagrams/flat-hill.pdf}
	\caption{A flat hill of new relays in 2009.  The time series was smoothed
	using a moving average with a window size of 12 hours.}
	\label{fig:flat-hill}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{diagrams/sudden-spike.pdf}
	\caption{A sudden spike of new relays in 2010.  The time series was smoothed
	using a moving average with a window size of 12 hours.}
	\label{fig:sudden-spike}
\end{figure}

To quantify the churn rate $\alpha$ between two subsequent consensus documents,
we adapt Godfrey et al.'s formula, which yields a churn value that captures
both systems that joined and systems that left the
network~\cite[\S~2.1]{Godfrey2006a}.  However, an unusually low number of
systems that left could cancel out an unusually high number of new systems and
vice versa---an undesired property for a technique that should spot abnormal
changes.  To address this issue, we split the formula in two parts, creating a
time series for new relays ($\alpha_{n}$) and for relays that left
($\alpha_{l}$).  $C_{t}$ is the network consensus at time $t$, and $\setminus$
denotes the complement between two consensuses, i.e., the relays that are in
the left operand, but not the right operand.  We define $\alpha_{n}$ and
$\alpha_{l}$ as

\begin{equation}
\alpha_{n} = \frac{\lvert C_{t} \setminus C_{t-1} \rvert}
{\lvert C_{t} \rvert}
\qquad\text{and}\qquad
\alpha_{l} = \frac{\lvert C_{t-1} \setminus C_{t} \rvert}
{\lvert C_{t-1} \rvert}.
\end{equation}

Both $\alpha_{n}$ and $\alpha_{l}$ are bounded to the interval $[0, 1]$.  A
churn value of 0 indicates no change between two subsequent consensuses whereas
a churn value of 1 indicates a complete turnover.  Determining $\alpha_{n,l}$
for the sequence $C_{t}, C_{t-1}$, \ldots, $C_{t-n}$, yields a time series of
churn values that can readily be inspected for abnormal spikes.
Figure~\ref{fig:churn-sensitivity} illustrates the maximum number of Sybils an
attacker can add to the network given a threshold for $\alpha$.  The figure
shows both the theoretical maximum and a more realistic estimate that accounts
for noise, i.e., the median number of new relays in each consensus, which is
73.\footnote{Note that this analysis is ``memoryless'' and includes relays that
have been online before; unlike the analysis above that considered only
previously unobserved relays, for which the median number was six.} We found
that many churn anomalies are caused by relays that share a flag, or a flag
combination, e.g., \texttt{HSDir} (onion service directories) and \texttt{Exit}
(exit relays).  Therefore, \sys can also generate per-flag churn time series
that can uncover patterns that would be lost in a flag-agnostic time series.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{diagrams/churn-sensitivity.pdf}
	\caption{The number of new Sybils ($y$ axis) that can remain undetected
	given a threshold for the churn value $\alpha$ ($x$ axis).  The diagram
	shows both the maximum and a more realistic estimate that accounts for the
	median number of new relays in consensuses.}
	\label{fig:churn-sensitivity}
\end{figure}

Finally, to detect changes in the underlying time series trend---flat hills---we
can smooth $\alpha_{n,l}$ using a simple moving average $\lambda$ defined as

\begin{equation}
\lambda = \frac{1}{w} \cdot \sum_{i=0}^{w} \alpha_{i}.
\end{equation}

As we increase the window size $w$, we can detect more subtle changes in the
underlying churn trend.  If $\lambda$ or $\alpha_{n,l}$ exceed a manually
defined threshold, an alert is raised.  Section~\ref{sec:churn} elaborates on
how we can select a threshold in practice.

\subsubsection{Uptime matrix}
\label{sec:uptime-matrix}
For convenience, Sybil operators are likely to administer their relays
simultaneously, i.e., update, configure, and reboot them all at the same time.
This is reflected in their relays' uptime.  An operating system upgrade that
requires a reboot of Sybil relays will induce a set of relays to go offline and
return online in a synchronized manner.  To isolate such events, we are
visualizing the \emph{uptime patterns} of Tor relays by grouping together relays
whose uptime is highly correlated.  The churn technique presented above is
similar but it only provides an aggregate, high-level view on how Tor relays
join and leave the network.  Since the technique is aggregate, it is poorly
suited for visualizing the uptime of specific relays; an abnormally high churn
value attracts our attention but does not tell us what caused the anomaly.  To
fill this gap, we complement the churn analysis with an uptime matrix that we
will now present.

This uptime matrix consists of the uptime patterns of all Tor relays, which we
represent as binary sequences.  Each hour, when a new consensus is published, we
add a new data point---``online'' or ``offline''---to each Tor relay's sequence.
We visualize all sequences in a bitmap whose rows represent consensuses and
whose columns represent relays.  Each pixel denotes the uptime status of a
particular relay at a particular hour.  Black pixels mean that the relay was
online and white pixels mean that the relay was offline.  This type of
visualization was first proposed by Ensafi and subsequently implemented by
Fifield~\cite{Fifield2014a}.

Of particular importance is how the uptime sequences are sorted.  If highly
correlated sequences are not adjacent in the visualization, we might miss them.
We sort sequences using single-linkage clustering, a type of hierarchical
clustering algorithm that forms groups bottom-up, based on the minimum distance
between group members.  For our distance function, similar to Andersen et
al.~\cite[\S~II.B]{Andersen2002a}, we use Pearson's correlation coefficient
because it tells us if two uptime sequences change together.  The sample
correlation coefficient $r$ yields a value in the interval $[-1, 1]$.  A
coefficient of $-1$ denotes perfect anti-correlation (relay $R_1$ is only
online when relay $R_2$ is offline) and 1 denotes perfect correlation (relay
$R_1$ is only online when relay $R_2$ is online).  We define our distance
function as $d(r) = 1 - r$, so two perfectly correlated sequences have a
distance of zero while two perfectly anti-correlated sequences have a distance
of two.  Once all sequences are sorted, we color five or more adjacent
sequences in red if their uptime sequence is identical.
Figure~\ref{fig:uptime-matrix} shows an example of our visualization algorithm,
the uptime matrix for a subset of all Tor relays in November 2012.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{diagrams/2012-11.jpg}
	\caption{The uptime matrix for 3,000 Tor relays for all of November 2012.
	Rows represent consensuses and columns represent relays.  Black pixels mean
	that a relay was online, and white means offline.  Red blocks denote relays
	with identical uptime.}
	\label{fig:uptime-matrix}
\end{figure}

\subsubsection{Fingerprint analysis}
\label{sec:fingerprint-analysis}
The information a Tor client needs to connect to an onion service is stored in a
DHT that consists of a subset of all Tor relays, the onion service directories
(HSDirs).  As of June 2016, 47\% of all Tor relays serve as HSDirs.  A
daily-changing set of six HSDirs hosts the contact information of any given
onion service.  Tor clients contact one of these six HSDirs to request
information about the onion service they intend to connect to.  A HSDir becomes
responsible for an onion service if the difference between its relay fingerprint
and the service's descriptor ID is smaller than that of any other relay.  The
descriptor ID is derived from the onion service's public key, a time stamp, and
additional information.
% https://gitweb.torproject.org/torspec.git/tree/rend-spec.txt:231
% descriptor-id = H(permanent-id | H(time-period | descriptor-cookie | replica))
All HSDirs are public, making it possible to determine at which position in the
DHT an onion service will end up at any point in the future.  Attackers can
exploit the ability to predict the DHT position by repeatedly generating
identity keys until their fingerprint is sufficiently close to the targeted
onion service's index, thus becoming its HSDir~\cite[\S~V.A]{Biryukov2013a}.

We detect relays that change their fingerprint frequently by maintaining a
lookup table that maps a relay's IP address to a list of all fingerprints we
have seen it use.  We sort the lookup table by the relays that changed their
fingerprints the most, and output the results.  Note that reboots or newly
assigned IP addresses are not an issue for this technique---as long as relays
do not lose their long-term keys that are stored on their hard drive, their
fingerprint stays the same.

\subsubsection{Nearest-neighbor ranking}
\label{sec:nearest-neighbor}
We frequently found ourselves in a situation where exitmap discovered a
malicious exit relay and we were left wondering if there were similar,
potentially associated relays.  Looking for such relays involved tedious manual
work, which we soon started to automate.  We needed an algorithm for
nearest-neighbor ranking that takes as input a ``seed'' relay and creates as
output a list of all relays, ranked by their similarity to the seed relay.  We
define similarity as shared configuration parameters such as port numbers, IP
addresses, exit policies, or bandwidth values.  Our algorithm ranks relays by
comparing these configuration parameters.

To quantify the similarity between two relays, we use the Levenshtein
distance~\cite{Levenshtein1966a}, a distance metric that takes as input two
strings and determines the minimum number of modifications---insert, delete,
and modify---that are necessary to turn string $s_{2}$ into $s_{1}$.  Our
algorithm turns the router statuses and descriptors of two relays into strings
and determines their Levenshtein distance.  As an example, consider a simple
representation consisting of the concatenation of nickname, IP address, and
port.  To turn string $s_2$ into $s_1$, six operations are necessary; four
modifications (green) and two deletions (red):

\definecolor{change}{rgb}{0.7,1.0,0.7}
\definecolor{delete}{rgb}{1.0,0.7,0.7}

$s_1$: \texttt{Foo10.0.0.19001}

$s_2$: \texttt{\setlength{\fboxsep}{0pt}%
\colorbox{change}{\strut Bar}10.0.0.%
\colorbox{change}{\strut 2}%
\colorbox{delete}{\strut 54}%
9001}

Our algorithm determines the Levenshtein distance between a ``seed'' relay and
all other relays in a consensus.  It then ranks the calculated distances in
ascending order.  For a consensus consisting of 6,525 relays, our algorithm
takes approximately 1.5 seconds to finish.\footnote{We measured on an Intel
Core i7-3520M CPU at 2.9 GHz, a consumer-grade CPU.}  Note that we designed our
ranking algorithm to assist in manual analysis.  Unlike the other analysis
techniques, it does not require a threshold.
